{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9afa101e-a9e5-4d1d-9e3e-416f1c767233",
   "metadata": {},
   "source": [
    "# makemore part 5: Building a WaveNet\n",
    "\n",
    "## What is makemore?\n",
    "\n",
    "Makemore \"makes more\" of things that you give it. Example uses `names.txt` and makemore learns to make names\n",
    "\n",
    "Under the hood, makemore is a character-level language model: it treats each line as sequences of individual characters. Model sequence of characters and try to predict next characters in a sequence.\n",
    "\n",
    "This class will look at \n",
    "1. Bigram (one character simply predicts a next one with a lookup table of counts)\n",
    "2. ~~Bag of Words~~\n",
    "   1. The table explodes! We'll skip\n",
    "3. **Multilayer Perceptron**\n",
    "4. Recurrent Neural Network\n",
    "5. GRU\n",
    "6. Transformers\n",
    "\n",
    "Will build a transformer equivalent to GPT-2, at the level of characters\n",
    "\n",
    "## Agenda\n",
    "Characters\n",
    "Words\n",
    "Images\n",
    "\n",
    "## WaveNet\n",
    "\n",
    "In this lecture, we will manually work through WaveNet because we have been compressing down to a single hidden layer with only a few characters. We will end up making WaveNet which is a much more complicated architecture.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef21b7d4-5980-4adf-a62b-38b8e89031ee",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b66d7b-46ad-4f10-9cb9-72c03e319fea",
   "metadata": {},
   "source": [
    "## External Resources\n",
    "\n",
    "### Lesson\n",
    "\n",
    "[Makemore part 5](https://www.youtube.com/watch?v=t3YJ5hKiMQ0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6a44e4-081e-4f8f-88d2-3eccd0f2d767",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1418bdb7-3ecf-4eb5-af6f-2c0e699d2394",
   "metadata": {},
   "source": [
    "## Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f480787a-b5ed-4bf2-8553-504d6cfffe17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt # for making figures\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a60dd0a-089e-41b7-9828-8f2b13648993",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28c3fb64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32033\n",
      "15\n",
      "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia', 'harper', 'evelyn']\n"
     ]
    }
   ],
   "source": [
    "# read in all the words\n",
    "names_data = '../../data/raw/names.txt'\n",
    "\n",
    "words = open(names_data, 'r').read().splitlines()\n",
    "\n",
    "print(len(words))\n",
    "print(max(len(w) for w in words))\n",
    "print(words[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "453979f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n",
      "27\n"
     ]
    }
   ],
   "source": [
    "# build the vocabulary of characters and mappings to/from integers\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i  in stoi.items()}\n",
    "vocab_size = len(itos)\n",
    "print(itos)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98db12d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle up the words \n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "83312728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182625, 8]) torch.Size([182625])\n",
      "torch.Size([22655, 8]) torch.Size([22655])\n",
      "torch.Size([22866, 8]) torch.Size([22866])\n"
     ]
    }
   ],
   "source": [
    "# build the dataset but via a function instead\n",
    "block_size = 8 # context length: how many characters do we take to predict the next one?\n",
    "    \n",
    "def build_dataset(words):\n",
    "    X, Y = [], []\n",
    "    \n",
    "    for w in words:\n",
    "        context = [0] * block_size\n",
    "        for ch in w + '.':\n",
    "            ix = stoi[ch]\n",
    "            X.append(context)\n",
    "            Y.append(ix)\n",
    "            context = context[1:] + [ix] # crop and append\n",
    "\n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    print(X.shape, Y.shape)\n",
    "    return X, Y\n",
    "\n",
    "n1 = int(0.8*len(words))\n",
    "n2 = int(0.9*len(words))\n",
    "\n",
    "Xtr, Ytr = build_dataset(words[:n1])        #80%\n",
    "Xdev, Ydev = build_dataset(words[n1:n2])    #10%\n",
    "Xte, Yte = build_dataset(words[n2:])        #10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "76f086bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "........ --> y\n",
      ".......y --> u\n",
      "......yu --> h\n",
      ".....yuh --> e\n",
      "....yuhe --> n\n",
      "...yuhen --> g\n",
      "..yuheng --> .\n",
      "........ --> d\n",
      ".......d --> i\n",
      "......di --> o\n",
      ".....dio --> n\n",
      "....dion --> d\n",
      "...diond --> r\n",
      "..diondr --> e\n",
      ".diondre --> .\n",
      "........ --> x\n",
      ".......x --> a\n",
      "......xa --> v\n",
      ".....xav --> i\n",
      "....xavi --> e\n"
     ]
    }
   ],
   "source": [
    "for x,y in zip(Xtr[:20], Ytr[:20]):\n",
    "    print(''.join(itos[ix.item()] for ix in x), '-->', itos[y.item()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad5f1481",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's train a deeper network\n",
    "\n",
    "class Linear:\n",
    "\n",
    "    def __init__(self, fan_in, fan_out, bias=True):\n",
    "        self.weight = torch.randn((fan_in, fan_out)) / fan_in**0.5\n",
    "        self.bias = torch.zeros(fan_out) if bias else None\n",
    "\n",
    "    def __call__(self, x):\n",
    "        self.out = x @ self.weight\n",
    "        if self.bias is not None:\n",
    "            self.out += self.bias\n",
    "        return self.out\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [self.weight] + ([] if self.bias is None else [self.bias])\n",
    "    \n",
    "\n",
    "class BatchNorm1d:\n",
    "\n",
    "    def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
    "        self.eps = eps\n",
    "        self.momentum = momentum\n",
    "        self.training = True\n",
    "\n",
    "        # parameters (trained with backprop)\n",
    "        self.gamma = torch.ones(dim)\n",
    "        self.beta = torch.zeros(dim)\n",
    "\n",
    "        # buffers (trained with a running 'momentum update')\n",
    "        self.running_mean = torch.zeros(dim)\n",
    "        self.running_var = torch.ones(dim)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        # calculate the forward pass\n",
    "        if self.training:\n",
    "            xmean = x.mean(0, keepdim=True)                 # batch mean\n",
    "            xvar = x.var(0, keepdim=True, unbiased=True)    # batch variance\n",
    "        else:\n",
    "            xmean = self.running_mean\n",
    "            xvar = self.running_var\n",
    "\n",
    "        xhat = (x - xmean) / (torch.sqrt(xvar + self.eps))  # normalizing to unit variance\n",
    "        self.out = self.gamma * xhat + self.beta\n",
    "\n",
    "        # update the buffers\n",
    "        if self.training:\n",
    "            with torch.no_grad():\n",
    "                self.running_mean = (1-self.momentum) * self.running_mean + self.momentum * xmean\n",
    "                self.running_var = (1-self.momentum) * self.running_var + self.momentum * xvar\n",
    "        return self.out\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [self.gamma, self.beta]\n",
    "\n",
    "class Tanh:\n",
    "    def __call__(self, x):\n",
    "        self.out = torch.tanh(x)\n",
    "        return self.out\n",
    "    \n",
    "    def parameters(self):\n",
    "        return []\n",
    "    \n",
    "class Embedding:\n",
    "    def __init__(self, num_embeddings, embedding_dim):\n",
    "        self.weight = torch.randn((num_embeddings, embedding_dim))\n",
    "\n",
    "    def __call__(self, IX):\n",
    "        self.out = self.weight[IX]\n",
    "        return self.out\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [self.weight]\n",
    "    \n",
    "class Flatten:\n",
    "    def __call__(self, x):\n",
    "        self.out = x.view(x.shape[0], -1)\n",
    "        return self.out\n",
    "    \n",
    "    def parameters(self):\n",
    "        return []\n",
    "    \n",
    "class Sequential:\n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        self.out = x\n",
    "        return self.out\n",
    "    \n",
    "    def parameters(self):\n",
    "        # get parameters of all layers and stretch them out into one list\n",
    "        return [p for layer in self.layers for p in layer.parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40390f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42); #seed rng for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5dbed2f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22097\n"
     ]
    }
   ],
   "source": [
    "# MLP revisited\n",
    "n_embd = 10 # the dimensionality of the character embedding vectors\n",
    "n_hidden = 200 # the number of neurons in the hidden layer of the MLP\n",
    "\n",
    "# this is before PyTorchifiying the code\n",
    "# C = torch.randn((vocab_size, n_embd))\n",
    "\n",
    "layers = [\n",
    "    Embedding(vocab_size, n_embd),\n",
    "    Flatten(),\n",
    "    Linear(n_embd * block_size, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
    "    Linear(n_hidden, vocab_size),\n",
    "]\n",
    "\n",
    "model = Sequential(layers)\n",
    "\n",
    "# parameter init\n",
    "with torch.no_grad():\n",
    "    layers[-1].weight *= 0.1 # make last layer less confident\n",
    "\n",
    "# this is before PyTorchifiying the code\n",
    "# parameters = [C] + [p for layer in layers for p in layer.parameters()]\n",
    "\n",
    "parameters = model.parameters()\n",
    "\n",
    "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d55386be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  0,  0,  0, 13,  1,  9, 19],\n",
       "        [ 0,  0,  0,  0,  7,  1, 18, 18],\n",
       "        [ 0,  0,  0,  0,  0,  0, 20,  1],\n",
       "        [ 0,  0,  0,  0,  0,  0,  0,  0]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's look at a batch of just 4 examples\n",
    "ix = torch.randint(0, Xtr.shape[0], (4,))\n",
    "Xb, Yb = Xtr[ix], Ytr[ix]\n",
    "logits = model(Xb)\n",
    "print(Xb.shape)\n",
    "Xb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b4724359",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 10])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# output of Embedding layer\n",
    "model.layers[0].out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c009a0ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 80])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# output of Flatten layer\n",
    "model.layers[1].out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "efc61533",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 200])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# output of Linear layer\n",
    "model.layers[2].out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cef35d33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 4, 20])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e = torch.randn(4, 8, 10)\n",
    "\n",
    "# goal: we want this to be (4,4,20) where consecutive 10-d vectors get concatenated\n",
    "explicit = torch.cat([e[:,::2,:], e[:, 1::2, :]], dim=2)\n",
    "explicit.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f959c940",
   "metadata": {},
   "outputs": [],
   "source": [
    "e.view(4, 4, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b682ead6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0 /  200000: 3.2966\n",
      "  10000 /  200000: 2.2322\n",
      "  20000 /  200000: 2.4111\n",
      "  30000 /  200000: 2.1004\n",
      "  40000 /  200000: 2.3157\n",
      "  50000 /  200000: 2.2104\n",
      "  60000 /  200000: 1.9653\n",
      "  70000 /  200000: 1.9767\n",
      "  80000 /  200000: 2.6738\n",
      "  90000 /  200000: 2.0837\n",
      " 100000 /  200000: 2.2730\n",
      " 110000 /  200000: 1.7087\n",
      " 120000 /  200000: 2.3243\n",
      " 130000 /  200000: 2.2512\n",
      " 140000 /  200000: 2.0113\n",
      " 150000 /  200000: 1.8195\n",
      " 160000 /  200000: 1.7985\n",
      " 170000 /  200000: 2.2206\n",
      " 180000 /  200000: 2.0566\n",
      " 190000 /  200000: 2.1030\n"
     ]
    }
   ],
   "source": [
    "max_steps = 200000\n",
    "batch_size = 32\n",
    "lossi = []\n",
    "\n",
    "for i in range(max_steps):\n",
    "    # minibatch construct\n",
    "    ix = torch.randint(0, Xtr.shape[0], (batch_size,))\n",
    "    Xb, Yb = Xtr[ix], Ytr[ix] # batch X, Y\n",
    "\n",
    "    # forward pass\n",
    "    # this is before PyTorchifiying the code\n",
    "    # emb = C[Xb] # embed the characters into vectors\n",
    "    # x = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
    "    # x = Xb\n",
    "    # for layer in layers:\n",
    "    #     x = layer(x)\n",
    "    logits = model(Xb)\n",
    "    # loss = F.cross_entropy(x, Yb)\n",
    "    loss = F.cross_entropy(logits, Yb)\n",
    "    \n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    # update: simple SGD\n",
    "    lr = 0.1 if i < 100000 else 0.01 # step learning rate decay\n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad\n",
    "\n",
    "    # track stats\n",
    "    if i % 10000 == 0:\n",
    "        print(f'{i:7d} / {max_steps:7d}: {loss.item():.4f}')\n",
    "    lossi.append(loss.log10().item())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6358b2b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plt.plot(lossi);\n",
    "plt.plot(torch.tensor(lossi).view(-1,1000).mean(1));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "57583891",
   "metadata": {},
   "outputs": [],
   "source": [
    "# put layers into eval mode (needed for BatchNorm especially)\n",
    "for layer in model.layers:\n",
    "    layer.training = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4d5c4cce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 3.2949328422546387\n",
      "val 3.2950856685638428\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad() # this decorator disables gradient tracking\n",
    "def split_loss(split):\n",
    "    x, y = {\n",
    "        'train': (Xtr, Ytr),\n",
    "        'val': (Xdev, Ydev),\n",
    "        'test': (Xte, Yte)\n",
    "    }[split]\n",
    "\n",
    "    # this is before PyTorchifiying the code\n",
    "    # emb = C[x] # (N, block_size, n_embd)\n",
    "    # x = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n",
    "    # for layer in layers:\n",
    "    #     x = layer(x)\n",
    "    \n",
    "    logits = model(x)\n",
    "    # loss = F.cross_entropy(x, y)\n",
    "    loss = F.cross_entropy(logits, y)\n",
    "    print(split, loss.item())\n",
    "\n",
    "split_loss('train')\n",
    "split_loss('val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b5f1041f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gygwj.\n",
      "ugrzphalyhnqlplghxnaagadtdfugdqqisuubtaoqzdzvxjvkxepfrvyuibmaugqrcoxxdzbplyfarizmvdofxpqatxljgvybwzj.\n",
      "kpjkthliyebtignjxlefnhzgbejpxghhifblwp.\n",
      "xfmposcdfablritdrnghn.\n",
      "qbhyzcavtnsskfubnmnlyp.\n",
      "dxfqdcftwt.\n",
      "wrt.\n",
      "vdkdzvfyjfbusaxpodvfnmycyqxwruubgnminrlbdrjsjg.\n",
      "gcedj.\n",
      "zovcvwc.\n",
      "pnznbymkxoqhpv.\n",
      "jevoogkcwfoqovembruacbofpehkrptrbsilemildchilaaexuwukyvycxxubhgigmsgamrwmaibhpzwzyowsojjvlblxtaytnua.\n",
      "aqvxgltxudlqzvobcs.\n",
      "gibrrxtkyfzdseldrkyrw.\n",
      "kclcxdmmnrpdjnigubuwgnlvmbwwxctaphbytkiexigw.\n",
      "iirvsecdfkkjuwebtjsmt.\n",
      "ekmpctqn.\n",
      "bohnqgprmufhexxyqvvxiavvzyzpcnkabxymgclmqqavljhmvgamltqwkuleqrfhjnxexdlnxlozrcuggkxtwuvqpkxregaoaueskjcarqxewezgwxzvnlcqffepb.\n",
      "nugomxrwirkecyulqvjgjmzsku.\n",
      "tptjacmcrktkoejwa.\n"
     ]
    }
   ],
   "source": [
    "# sample from the model\n",
    "\n",
    "for _ in range(20):\n",
    "    out = []\n",
    "    context = [0] * block_size #initialize with all ...\n",
    "    while True:\n",
    "        # forward pass the neural net\n",
    "        # this is before PyTorch-ifying the code\n",
    "        # emb = C[torch.tensor([context])] # (1, block_size, n_embd)\n",
    "        # x = emb.view(emb.shape[0], -1)\n",
    "        # for layer in layers:\n",
    "        #     x = layer(x)\n",
    "        # logits = x\n",
    "\n",
    "        logits = model(torch.tensor([context]))\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        \n",
    "        # sample from the distribution\n",
    "        ix = torch.multinomial(probs, num_samples=1).item()\n",
    "\n",
    "        # shift the context window and track the samples\n",
    "        context = context[1:] + [ix]\n",
    "        out.append(ix)\n",
    "\n",
    "        # if we sample the special '.' token, break\n",
    "        if ix == 0:\n",
    "            break\n",
    "    \n",
    "    print(''.join(itos[i] for i in out)) #decode and print the generated word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcfa5f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7fa36c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40723b0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d82b533",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb59fdab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility function we will use later when comparing manual gradients to PyTorch gradients\n",
    "def cmp(s, dt, t):\n",
    "    ex = torch.all(dt == t.grad).item()\n",
    "    app = torch.allclose(dt, t.grad)\n",
    "    maxdiff = (dt - t.grad).abs().max().item()\n",
    "    print(f'{s:15s} | exact: {str(ex):5s} | approximate: {str(app):5s} | maxdiff: {maxdiff}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02860e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "n = batch_size # a shorter variable also, for convenience\n",
    "\n",
    "# construct a minibatch\n",
    "ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa64c3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward pass, \"chunkated\" into smaller steps that are possible to backward one at a time\n",
    "\n",
    "emb = C[Xb] # embed the characters into vectors\n",
    "embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
    "\n",
    "# Linear layer 1\n",
    "hprebn = embcat @ W1 + b1 # hidden layer pre-activation\n",
    "\n",
    "# BatchNorm layer \n",
    "bnmeani = 1/n*hprebn.sum(0, keepdim=True)\n",
    "bndiff = hprebn - bnmeani\n",
    "bndiff2 = bndiff**2\n",
    "bnvar = 1/(n-1)*(bndiff2).sum(0,keepdim=True) # note: Bessel's corection (dividing by n-1, not n)\n",
    "bnvar_inv = (bnvar + 1e-5)**-0.5\n",
    "bnraw = bndiff * bnvar_inv\n",
    "hpreact = bngain * bnraw + bnbias\n",
    "\n",
    "# non-linearity\n",
    "h = torch.tanh(hpreact) # hidden layer\n",
    "\n",
    "# linear layer 2\n",
    "logits = h @ W2 + b2 # output layer\n",
    "\n",
    "# cross entropy loss (same as F.cross_entropy(logits, Yb))\n",
    "logit_maxes = logits.max(1, keepdim=True).values\n",
    "norm_logits = logits - logit_maxes # subtract max for numerical stability\n",
    "counts = norm_logits.exp()\n",
    "counts_sum = counts.sum(1, keepdims=True)\n",
    "counts_sum_inv = counts_sum**-1 # if I use (1.0 / counts_sum) isntead then I can't get backprop to be bit exact...\n",
    "probs = counts * counts_sum_inv\n",
    "logprobs = probs.log()\n",
    "loss = -logprobs[range(n), Yb].mean()\n",
    "\n",
    "# PyTorch backward pass\n",
    "for p in parameters:\n",
    "    p.grad = None\n",
    "for t in [logprobs, probs, counts, counts_sum, counts_sum_inv, # afaik there is no cleaner way\n",
    "          norm_logits, logit_maxes, logits, h, hpreact, bnraw, \n",
    "          bnvar_inv, bnvar, bndiff2, bndiff, hprebn, bnmeani, \n",
    "          embcat, emb]:\n",
    "    t.retain_grad()\n",
    "loss.backward()\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6c9d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "logprobs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b93326",
   "metadata": {},
   "outputs": [],
   "source": [
    "Yb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1e9a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "logprobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44bfc14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss = -(a + b + c) / 3\n",
    "# dloss/da = -1/3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1e8573",
   "metadata": {},
   "source": [
    "### Exercise 1: \n",
    "\n",
    "Backprop through the whole thing manually, backpropagating through exactly all of the variables as they are defined in the forward pass above, one by one\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d0e6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "dlogprobs = torch.zeros_like(logprobs)\n",
    "dlogprobs[range(n), Yb] = -1.0/n\n",
    "\n",
    "cmp('logprobs', dlogprobs, logprobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8c016f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dprobs = (1.0 / probs) * dlogprobs\n",
    "cmp('probs', dprobs, probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecbec2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts.shape, counts_sum_inv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75ca9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dcounts_sum_inv = (counts * dprobs).sum(1, keepdim=True)\n",
    "cmp('counts_sum_inv', dcounts_sum_inv, counts_sum_inv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9264ed4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dcounts_sum = (-counts_sum**-2) * dcounts_sum_inv\n",
    "cmp('counts_sum', dcounts_sum, counts_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604b24cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dcounts = counts_sum_inv * dprobs\n",
    "dcounts += torch.ones_like(counts) * dcounts_sum\n",
    "cmp('counts', dcounts, counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbb2ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dnorm_logits = counts * dcounts\n",
    "cmp('norm_logits', dnorm_logits, norm_logits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e164498",
   "metadata": {},
   "outputs": [],
   "source": [
    "dlogit_maxes = (-dnorm_logits).sum(1, keepdim=True)\n",
    "cmp('logit_maxes', dlogit_maxes, logit_maxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5ef73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(F.one_hot(logits.max(1).indices, num_classes=logits.shape[1]));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed720e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dlogits = dnorm_logits.clone()\n",
    "dlogits += F.one_hot(logits.max(1).indices, num_classes=logits.shape[1]) * dlogit_maxes\n",
    "cmp('logits', dlogits, logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca7a7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dh = dlogits @ W2.T \n",
    "dW2 = h.T @ dlogits\n",
    "db2 = dlogits.sum(0)\n",
    "\n",
    "cmp('h', dh, h)\n",
    "cmp('W2', dW2, W2)\n",
    "cmp('b2', db2, b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04224667",
   "metadata": {},
   "outputs": [],
   "source": [
    "dhpreact = (1.0 - h**2) * dh\n",
    "cmp('hpreact', dhpreact, hpreact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd37cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbngain = (bnraw * dhpreact).sum(0, keepdim=True)\n",
    "dbnraw = bngain * dhpreact\n",
    "dbnbias = dhpreact.sum(0, keepdim=True)\n",
    "\n",
    "cmp('bngain', dbngain, bngain)\n",
    "cmp('bnbias', dbnbias, bnbias)\n",
    "cmp('bnraw', dbnraw, bnraw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203b1a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbndiff = bnvar_inv * dbnraw\n",
    "dbnvar_inv = (bndiff * dbnraw).sum(0, keepdim=True)\n",
    "dbnvar = (-0.5*(bnvar + 1e-5)**-1.5) * dbnvar_inv\n",
    "\n",
    "cmp('bnvar_inv', dbnvar_inv, bnvar_inv)\n",
    "cmp('bnvar', dbnvar, bnvar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff70392",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbndiff2 = (1.0/(n-1))*torch.ones_like(bndiff2) * dbnvar\n",
    "cmp('bndiff2', dbndiff2, bndiff2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70163a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbndiff += (2*bndiff) * dbndiff2\n",
    "cmp('bndiff', dbndiff, bndiff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34a8f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dhprebn = dbndiff.clone()\n",
    "dbnmeani = (-dbndiff).sum(0)\n",
    "\n",
    "cmp('bnmeani', dbnmeani, bnmeani)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac5717e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dhprebn += 1.0/n * (torch.ones_like(hprebn) * dbnmeani) \n",
    "cmp('hprebn', dhprebn, hprebn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381c4d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "dembcat = dhprebn @ W1.T\n",
    "dW1 = embcat.T @ dhprebn\n",
    "db1 = dhprebn.sum(0)\n",
    "\n",
    "cmp('embcat', dembcat, embcat)\n",
    "cmp('W1', dW1, W1)\n",
    "cmp('b1', db1, b1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85c0101",
   "metadata": {},
   "outputs": [],
   "source": [
    "demb = dembcat.view(emb.shape)\n",
    "\n",
    "cmp('emb', demb, emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5419c99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dC = torch.zeros_like(C)\n",
    "for k in range(Xb.shape[0]):\n",
    "    for j in range(Xb.shape[1]):\n",
    "        ix = Xb[k,j]\n",
    "        dC[ix] += emb[k,j] \n",
    "cmp('C', dC, C)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ec97fa",
   "metadata": {},
   "source": [
    "### Exercise 2:\n",
    "\n",
    "Backprop through cross_entropy but all in one go. To complete this challenge look at the mathematical expression of the loss, take the derivative, simplify the expression, and just write it out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87832d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward pass\n",
    "\n",
    "# before:\n",
    "logit_maxes = logits.max(1, keepdim=True).values\n",
    "norm_logits = logits - logit_maxes # subtract max for numerical stability\n",
    "counts = norm_logits.exp()\n",
    "counts_sum = counts.sum(1, keepdims=True)\n",
    "counts_sum_inv = counts_sum**-1 # if I use (1.0 / counts_sum) isntead then I can't get backprop to be bit exact...\n",
    "probs = counts * counts_sum_inv\n",
    "logprobs = probs.log()\n",
    "loss = -logprobs[range(n), Yb].mean()\n",
    "\n",
    "# now:\n",
    "loss_fast = F.cross_entropy(logits, Yb)\n",
    "print(loss_fast.item(), 'diff: ', (loss_fast - loss).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbb2b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# backward pass\n",
    "\n",
    "dlogits = F.softmax(logits, 1)\n",
    "dlogits[range(n), Yb] -= 1\n",
    "dlogits /= n\n",
    "\n",
    "cmp('logits', dlogits, logits) # I can only get approximate to be true, my maxdiff is 6e-9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640a8dcb",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "\n",
    "Back prop through BatchNorm but all in one go. To complete this challenge, look at the mathematical expression of the output of BatchNorm, take the derivative w.r.t. its input, simplify the expression, and just write it out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3df0a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward pass\n",
    "\n",
    "# before:\n",
    "# bnmeani = 1/n*hprebn.sum(0, keepdim=True)\n",
    "# bndiff = hprebn - bnmeani\n",
    "# bndiff2 = bndiff**2\n",
    "# bnvar = 1/(n-1)*(bndiff2).sum(0,keepdim=True) # note: Bessel's corection (dividing by n-1, not n)\n",
    "# bnvar_inv = (bnvar + 1e-5)**-0.5\n",
    "# bnraw = bndiff * bnvar_inv\n",
    "# hpreact = bngain * bnraw + bnbias\n",
    "\n",
    "# now:\n",
    "hpreact_fast = bngain * (hprebn - hprebn.mean(0, keepdim=True)) / torch.sqrt(hprebn.var(0,keepdim=True, correction=1))\n",
    "print('max diff:', (hpreact_fast - hpreact).abs().max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2922c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# backward pass\n",
    "\n",
    "# before \n",
    "# almost the entirety of exercise 1!\n",
    "\n",
    "# calculate dhprebn given dhpreact (i.e. backprop through the batchnorm)\n",
    "# you'll also need to use some of the variables from the forward pass up above\n",
    "\n",
    "dhprebn = bngain * bnvar_inv / n * (n*dhpreact - dhpreact.sum(0) - n /(n-1) * bnraw * (dhpreact*bnraw).sum(0))\n",
    "cmp('hprebn', dhprebn, hprebn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f175865b",
   "metadata": {},
   "source": [
    "### Exercise 4: Putting it all together!\n",
    "\n",
    "Train the MLP neural net with your own backward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab9d580",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init\n",
    "n_embd = 10 # the dimensionality of the character embedding vectors\n",
    "n_hidden = 200 # the number of neurons in the hidden layer of the MLP\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "C = torch.randn((vocab_size, n_embd), generator=g)\n",
    "\n",
    "# Layer 1\n",
    "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3) / ((n_embd * block_size)**0.5)\n",
    "b1 = torch.randn(n_hidden, generator=g) * 0.1 # using b1 just for fun, it's useless because of the BatchNormalization layer\n",
    "\n",
    "# Layer 2\n",
    "W2 = torch.randn((n_hidden,vocab_size), generator=g) * 0.1\n",
    "b2 = torch.randn(vocab_size, generator=g) * 0.1\n",
    "\n",
    "# BatchNorm parameters\n",
    "bngain = torch.ones((1, n_hidden)) * 0.1 + 1.0\n",
    "bnbias = torch.zeros((1, n_hidden)) * 0.1\n",
    "\n",
    "# Note: I am initializing many of these parameters in non-standard ways because sometimes initializing with e.g. all zeros could mask an incorrect implementation of the backward pass\n",
    "# bnmean_running = torch.zeros((1, n_hidden))\n",
    "# bnstd_running = torch.ones((1, n_hidden))\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
    "\n",
    "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
    "for p in parameters:\n",
    "    p.requires_grad = True\n",
    "\n",
    "# same optimization as last time\n",
    "max_steps = 200000\n",
    "batch_size = 32\n",
    "n = batch_size # convenience\n",
    "lossi = []\n",
    "\n",
    "# use this context manager for efficiency once your backward pass is written (TODO)\n",
    "with torch.no_grad():\n",
    "\n",
    "# kick off optimization\n",
    "    for i in range(max_steps):\n",
    "        # minibatch construct\n",
    "        ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "        Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n",
    "        \n",
    "        emb = C[Xb] # embed the characters into vectors\n",
    "        embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
    "\n",
    "        # Linear layer 1\n",
    "        hprebn = embcat @ W1 + b1 # hidden layer pre-activation\n",
    "\n",
    "        # BatchNorm layer \n",
    "        bnmean = hprebn.mean(0, keepdim=True)\n",
    "        bnvar = hprebn.var(0, keepdim=True, correction=1)\n",
    "        bnvar_inv = (bnvar + 1e-5)**-0.5\n",
    "        bnraw = (hprebn - bnmean) * bnvar_inv\n",
    "        hpreact = bngain * bnraw + bnbias\n",
    "\n",
    "        # non-linearity\n",
    "        h = torch.tanh(hpreact) # hidden layer\n",
    "\n",
    "        # linear layer 2\n",
    "        logits = h @ W2 + b2 # output layer\n",
    "\n",
    "        # cross entropy loss (same as F.cross_entropy(logits, Yb))\n",
    "        loss = F.cross_entropy(logits, Yb) # loss function\n",
    "\n",
    "        # PyTorch backward pass\n",
    "        for p in parameters:\n",
    "            p.grad = None\n",
    "        \n",
    "        # loss.backward() #use this for correctness comparisons, delete it later!\n",
    "        \n",
    "        # manual backprop! #swole_doge_meme\n",
    "        dlogits = F.softmax(logits, 1)\n",
    "        dlogits[range(n), Yb] -= 1\n",
    "        dlogits /= n\n",
    "\n",
    "        # 2nd layer backprop\n",
    "        dh = dlogits @ W2.T \n",
    "        dW2 = h.T @ dlogits\n",
    "        db2 = dlogits.sum(0)\n",
    "\n",
    "        # tanh\n",
    "        dhpreact = (1.0 - h**2) * dh\n",
    "\n",
    "        # batchnorm backprop\n",
    "        dbngain = (bnraw * dhpreact).sum(0, keepdim=True)\n",
    "        dbnraw = bngain * dhpreact\n",
    "        dbnbias = dhpreact.sum(0, keepdim=True)\n",
    "\n",
    "        # 1st layer\n",
    "        dembcat = dhprebn @ W1.T\n",
    "        dW1 = embcat.T @ dhprebn\n",
    "        db1 = dhprebn.sum(0)\n",
    "\n",
    "        # embedding\n",
    "        demb = dembcat.view(emb.shape)\n",
    "        dC = torch.zeros_like(C)\n",
    "\n",
    "        for k in range(Xb.shape[0]):\n",
    "            for j in range(Xb.shape[1]):\n",
    "                ix = Xb[k,j]\n",
    "                dC[ix] += demb[k,j]\n",
    "        \n",
    "        grads = [dC, dW1, db1, dW2, db2, dbngain, dbnbias]\n",
    "\n",
    "        # update\n",
    "        lr = 0.1 if i < 100000 else 0.01 # step learning rate decary\n",
    "        for p, grad in zip(parameters, grads):\n",
    "            # p.data += -lr * p.grad # old way of cheems doge (using PyTorch grad from .backward())\n",
    "            p.data += -lr * grad # new way of swole doge TODO: enable\n",
    "\n",
    "        # track stats\n",
    "        if i % 10000 == 0: # print every once in a while\n",
    "            print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
    "        lossi.append(loss.log10().item())\n",
    "\n",
    "        # if i >= 100: #TODO: early breaking when you're ready to train the full net\n",
    "        #     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee81c276",
   "metadata": {},
   "outputs": [],
   "source": [
    "# useful for checking your gradients\n",
    "# for p,g in zip(parameters, grads):\n",
    "#     cmp(str(tuple(p.shape)), g, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867c86db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calibrate the batch norm at the end of training\n",
    "with torch.no_grad():\n",
    "    # pass the training set through\n",
    "    emb = C[Xtr]\n",
    "    embcat = emb.view(emb.shape[0], -1)\n",
    "    hpreact = embcat @ W1 + b1\n",
    "    \n",
    "    # measure the mean/std over the entire training set\n",
    "    bnmean = hpreact.mean(0, keepdim=True)\n",
    "    bnvar = hpreact.var(0, keepdim=True, correction=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa69bd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate train and val loss\n",
    "@torch.no_grad() # this decorator disables gradient tracking\n",
    "def split_loss(split):\n",
    "    x, y = {\n",
    "        'train': (Xtr, Ytr),\n",
    "        'val': (Xdev, Ydev),\n",
    "        'test': (Xte, Yte)\n",
    "    }[split]\n",
    "\n",
    "    emb = C[x]\n",
    "    embcat = emb.view(emb.shape[0], -1)\n",
    "    hpreact = embcat @ W1 + b1\n",
    "    hpreact = bngain * (hpreact - bnmean) * (bnvar + 1e-5)**-0.5 + bnbias\n",
    "    h = torch.tanh(hpreact) # (N, n_hidden)\n",
    "    logits = h @ W2 + b2 # (N, vocab_size)\n",
    "    loss = F.cross_entropy(logits, y)\n",
    "    print(split, loss.item())\n",
    "\n",
    "split_loss('train')\n",
    "split_loss('val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4717e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(20):\n",
    "    out = []\n",
    "    context = [0] * block_size # initialize with all ...\n",
    "    while True:\n",
    "        # forward pass\n",
    "        # Embedding\n",
    "        emb = C[torch.tensor([context])] # (1, block_size, d)\n",
    "        embcat = emb.view(emb.shape[0], -1)\n",
    "        hpreact = embcat @ W1 + b1\n",
    "        hpreact = bngain * (hpreact - bnmean) * (bnvar + 1e-5)**-0.5 + bnbias\n",
    "        h = torch.tanh(hpreact) # (N, n_hidden)\n",
    "        logits = h @ W2 + b2\n",
    "\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n",
    "        context = context[1:] + [ix]\n",
    "        out.append(ix)\n",
    "        if ix == 0:\n",
    "            break\n",
    "\n",
    "        print(''.join(itos[i] for i in out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c01ea5f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
